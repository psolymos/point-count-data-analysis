---
title: "Naïve estimates of occupancy and abundance"
subtitle: "Point count data analysis workshop 2025"
date: "`r Sys.Date()`"
author: "Péter Sólymos"
toc: true
format:
  html: 
    html-math-method: katex
    self-contained: true
  pdf: default
---

```{r}
#| include: false
library(knitr)
```

# Preamble

```{r}
suppressPackageStartupMessages({
    library(dplyr)
    library(ggplot2)
    library(mefa4)
    library(detect)
})
```

# Nuisance variables

Nuisance variables are covariates whose effects we want to control
but we are not really interested in their effect. We just need to
estimate them so that we can account for them as best as we can.

We have 3 such covariates in our example:

- `TSSR`: time since local sunrise, (survey time - sunrise time) / 24
- `DAY`: day of the year, ordinal day / 365
- `WindStart`: wind speed using [Beaufort scale](https://en.wikipedia.org/wiki/Beaufort_scale)

```{r}
x <- detect::josm$surveys |>
    select(
        Longitude,
        Latitude,
        WindStart,
        TSSR,
        DAY,
        Open,
        Water,
        Decid,
        OpenWet,
        Conif,
        ConifWet,
        Agr,
        UrbInd,
        SoftLin,
        Roads
    )
x[is.na(x$WindStart), "WindStart"] <- c(0, 3)

spp <- "OVEN" # change here if you want to use another species

y <- mefa4::Xtab(~ SiteID + SpeciesID, detect::josm$counts)[, spp, drop = FALSE]
x$Count <- y[rownames(x), ]
```


```{r}
m1 <- glm(Count ~ Decid * ConifWet + TSSR + I(TSSR^2) + DAY + WindStart, data = x, family = poisson)
summary(m1)

m2 <- step(m1, trace = 0)
summary(m2)

MuMIn::model.sel(m1, m2)
```

When predicting abundance and accounting for nuisance variables,
we want to set the values that maximize the prediction.
All 3 nuisance variables have a negative effect, so we need to pick the mim:

```{r}
xnew <- x
xnew$TSSR <- min(xnew$TSSR)
xnew$DAY <- min(xnew$DAY)
xnew$WindStart <- min(xnew$WindStart)

x$Pred <- predict(m2, x, type = "response")
x$PredNew <- predict(m2, xnew, type = "response")
mean(x$Pred)
mean(x$PredNew)

x |> ggplot(aes(x = Decid, y = Pred, col = WindStart)) +
    geom_point() +
    theme_light()

x |> ggplot(aes(x = Decid, y = PredNew)) +
    geom_point() +
    theme_light()

x |> ggplot(aes(x = Pred, y = PredNew, col = WindStart)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, lty = 2) +
    theme_light()
```

## Offsets

Offsets are constant terms in the linear predictor, e.g. 
$log(\lambda_i) = \beta_0 + \beta_1 x_{1i} + o_i$, where $o_i$ is an offset.

In the survey area case, an offset might be the log of area surveyed. 
Abundance ($N$) is population density ($D$) multiplied by survey area ($A$). 
The logic for this is based on point processes: intensity is a linear function 
of area under a homogeneous Poisson point process. So we can say that 
$E[Y_i] = N_i = D_i A_i$, thus $log(N_i) = log(D_i) + log(A_i)$ 
where $o_i = log(A_i)$ is the offset.

The problem is that we do not know the survey area. We used 10 minutes and
unlimited distance counts.


Let's see if using area as offset makes our models comparable. Instead of mixing up different survey types, let's see if we can make them identical. We use distance in meters divided by 100, so the population density is estimated in ha.

## Distance effects

We will inspect what happens if we used different survey radii.
Our data set has 50 and 100 m distances. We will not use the unlimited (>100 m)
radius because the area surveyed would be arbitrary. On Day 2 we will see
how to estimate the effective area sampled instead.

First, the `y1` matrix will have columns for the counts in the 3
different distance bands that we used to add 2 new `Count` column:

```{r}
y1 <- mefa4::Xtab(~ SiteID + Dis, detect::josm$counts, subset = detect::josm$counts$SpeciesID == spp)
head(y1)

x$Count_10min_50m <- y1[rownames(x), "0-50m"]
x$Count_10min_100m <- y1[rownames(x), "0-50m"] + y1[rownames(x), "50-100m"]
```

Not surprisingly, the mean count in the 0--50 m area and much smaller than 
the counts in the 0--100 m area:

```{r}
mean(y1[, "0-50m"])
mean(y1[, "0-50m"] + y1[, "50-100m"])
```

The mean density (abundance per unit area), however, shows the reverse pattern.
Here we used units of 100 m, e.g. 50 m becomes 0.5, and 100 m becomes 1.
This way, the density is calculated over a 100 x 100 $m^2$ area, that is 1 ha,
commonly used for expressing density in the bird literature:

```{r}
mean(y1[, "0-50m"]) / (0.5^2 * pi)
mean(y1[, "0-50m"] + y1[, "50-100m"]) / (1^2 * pi)
```

Why would the density over the 100 m area be smaller than the density over the
50 m area? Take your guesses. We'll learn about this in the next 2 days.

We add the survey area is calculated as $A = r^2 \pi$ and then take the log of
if because we are using the log link function. We then update the formula
to use `Count_10min_50m` on the left and `offset(logA_50m)` on the right:

```{r}
x$logA_50m <- log(0.5^2 * pi)
m3 <- update(m2, Count_10min_50m ~ . + offset(logA_50m))
summary(m3)
```

We follow the same steps for the 100 m radius model:

```{r}
x$logA_100m <- log(1^2 * pi)
m4 <- update(m2, Count_10min_100m ~ . + offset(logA_100m))
summary(m4)
```

As for predicting with an offset, we will create a copy of `xnew`
and add the 2 offset terms. Notice that we don't use the same values
as we used for modeling, because we want predictions over a unit area of 1 ha,
which is $log(1)=0$.

```{r}
xnew2 <- xnew

xnew2$logA_50m <- 0
xnew2$logA_100m <- 0

xnew2$Pred_10min_50m <- predict(m3, xnew2, type = "response")
xnew2$Pred_10min_100m <- predict(m4, xnew2, type = "response")
```

Density per unit area is higher for the 50 m model, similarly to what we have 
seen before.

```{r}
mean(xnew2$Pred_10min_50m)
mean(xnew2$Pred_10min_100m)
```

Let's compare the predictions:

```{r}
xnew2 |> ggplot(aes(x = Pred_10min_50m, y = Pred_10min_100m)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, lty = 2) +
    theme_light()
```

## Duration effect

Next we will inspect what effect the count duration has on our estimates.
Follow the same approach to get the counts by sites and distance intervals
after subsetting for our species and for the 0--3 minutes time interval:

```{r}
y2 <- mefa4::Xtab(~ SiteID + Dis, detect::josm$counts,
    subset = detect::josm$counts$SpeciesID == spp &
        detect::josm$counts$Dur == "0-3min"
)
x$Count_3min_50m <- y2[rownames(x), "0-50m"]
x$Count_3min_100m <- y2[rownames(x), "0-50m"] + y2[rownames(x), "50-100m"]
```

Same patterns for mean count and the empirical density as what we saw based on
the 10 minutes counts:

```{r}
mean(y2[, "0-50m"])
mean(y2[, "0-50m"] + y2[, "50-100m"])

mean(y2[, "0-50m"]) / (0.5^2 * pi)
mean(y2[, "0-50m"] + y2[, "50-100m"]) / (1^2 * pi)
```

The models:

```{r}
m5 <- update(m2, Count_3min_50m ~ . + offset(logA_50m))
summary(m5)

m6 <- update(m2, Count_3min_100m ~ . + offset(logA_100m))
summary(m6)
```

Predictions using `xnew2` as before:

```{r}
xnew2$Pred_3min_50m <- predict(m5, xnew2, type = "response")
xnew2$Pred_3min_100m <- predict(m6, xnew2, type = "response")
```

The 10 minutes estimates are higher due to accumulating more detections
over the longer duration; the 50 m radius counts led to higher density estimates
due to higher detectability compared to the 100 m radius counts:

```{r}
D_est <- c(
    "10min 50m" = mean(xnew2$Pred_10min_50m),
    "10min 100m" = mean(xnew2$Pred_10min_100m),
    "3min 50m" = mean(xnew2$Pred_3min_50m),
    "3min 100m" = mean(xnew2$Pred_3min_100m)
)
D_est

barplot(D_est)
```

Make some plots to compare the site level predictions:

```{r}
xnew2 |> ggplot(aes(x = Pred_3min_50m, y = Pred_3min_100m)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, lty = 2) +
    theme_light()

xnew2 |> ggplot(aes(x = Pred_3min_50m, y = Pred_10min_50m)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, lty = 2) +
    theme_light()

xnew2 |> ggplot(aes(x = Pred_3min_100m, y = Pred_10min_100m)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, lty = 2) +
    theme_light()
```

## Accounting for different field protocols

When we survey some areas, the survey protocol is the same for all sites.
However, when we combine different datasets, we might find that
each might have a slightly different sampling design.

Some surveys might be 10 min (3--5--100 min) and unlimited radius
(0--100--$\infty$ m), other might be 0--3 min and unlimited (0--$\infty$ m)
without any time and distance bands.

We will assign one of 4 survey protocols to each site and based on that:

- the `Count` values corresponding to the time and distance intervals
- set the log area for offset matching the protocol radii


```{r}
Methods <- c("3min_50m", "3min_100m", "10min_50m", "10min_100m")
x$Method <- factor(sample(
    Methods,
    nrow(x),
    replace = TRUE
), levels = Methods)
x$Count_Rnd <- 0
for (i in Methods) {
    x$Count_Rnd[x$Method == i] <- x[[paste0("Count_", i)]][x$Method == i]
}
x$logA_Rnd <- log(ifelse(x$Method %in% c("3min_50m", "10min_50m"), 0.5, 1)^2 * pi)
```

This next model will have `Count_Rnd` as the response, and we add
the `Method` variable and the offset `offset(logA_Rnd)`:

```{r}
m7 <- update(m2, Count_Rnd ~ . + Method + offset(logA_Rnd))
summary(m7)
```

When predicting, we pick the method that leads to highest counts (10 min) and
smalles detection error (50 m radius), define the log area offset as 0:

```{r}
xnew2$logA_Rnd <- 0
xnew2$Method <- "10min_50m" # highest duration, highest detection
xnew2$Pred_Rnd <- predict(m7, xnew2, type = "response")

mean(xnew2$Pred_Rnd)
```

The mean density is 0.46 male birds per ha, but it can reach a maximum of 1.1
males per ha for fully deciduous habitats:

```{r}
xnew2 |> ggplot(aes(x = Decid, y = Pred_Rnd)) +
    geom_point() +
    geom_smooth() +
    theme_light() +
    ylab("Naive density [male individuals/ha]")
```

# Mapping results

The habitat covariates came from detailed (vector based) habitat mapping,
these were than summarized by 1 $km^2$ pixels using the area proportions
of these land cover types. The raster file in the `data` folder
has the corresponding layers. 

```{r}
library(sf)
library(terra)

r <- rast("../data/josm-landcover.tif")

plot(r, col = hcl.colors(50, "Lajolla"), axes = FALSE)
```

By fixing the nuisance variables to a constant
value and adding our offset term, we can make predictions.

Without detailing all the steps, here is how to make raster predictions
using `m7`:

```{r}
rd <- data.frame(
    as.matrix(r),
    WindStart = min(x$WindStart),
    TSSR = min(x$TSSR),
    DAY = min(x$DAY),
    Method = "10min_50m",
    logA_Rnd = 0
)

rp <- r[[1]]
names(rp) <- "Count"
values(rp) <- predict(m7, rd, type = "response")

plot(rp, axes = FALSE)
```

Next we add latitude and longitude (with interactions and quadratic terms)
as new terms to better capture spatial patterns:

```{r}
xy <- sf::st_as_sf(x, coords = c("Longitude", "Latitude"), crs = 4269) # NAD83 EPSG:4269
## NAD83 / Alberta 10-TM (Forest) EPSG:3400
xy <- sf::st_transform(xy, sf::st_crs(r))
coords <- sf::st_coordinates(xy)
x$X <- coords[, "X"]
x$Y <- coords[, "Y"]

m8 <- update(m7, . ~ . + X * Y + I(X^2) * I(Y^2))

rp2 <- rp
rxy <- crds(r, na.rm = FALSE)
colnames(rxy) <- c("X", "Y")
rd <- data.frame(rd, rxy)
values(rp2) <- predict(m8, rd, type = "response")

plot(rast(list(rp, rp2)), axes = FALSE)

mean(values(rp), na.rm = TRUE)
mean(values(rp2), na.rm = TRUE)

2 * sum(values(rp), na.rm = TRUE) * 100
2 * sum(values(rp2), na.rm = TRUE) * 100
```


```{r}
#| eval: FALSE
library(mapview)

ct <- xy[, "Count"]
ct$Count[ct$Count > 0] <- 1
ct$Count <- as.factor(ct$Count)

mapview(rp2) + mapview(ct)
```

# Next

_Multiple-visit occupancy and N-mixture models_
