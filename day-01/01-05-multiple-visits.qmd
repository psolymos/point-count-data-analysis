---
title: "Multiple-visit occupancy and N-mixture models"
date: "`r Sys.Date()`"
author: "Péter Sólymos"
toc: true
format:
  html: 
    html-math-method: katex
    self-contained: true
---

```{r}
#| include: false
options(width = 53, scipen = 999)
library(knitr)
set.seed(0)
library(dplyr)
library(ggplot2)
```

# Introduction to simulations

The goal is to implement

- a data generating mechanisms
- using random numbers

Here is the most basic simulation: the coin flip.

We assume that the coin is fair, therefore:

- the probability of getting head ($y=1$) is $p=0.5$
- the probability of getting tail ($y=0$) is $1-p=0.5$

Here is code for setting the probability value $p$
and getting the outcome $y$ using Uniform random numbers:

1. We generate a random number (`u`) between 0 and 1
2. The outcome is 1 if the number is $<p$
3. The outcome is 0 if the random number of $\ge p$

```{r}
p <- 0.5

u <- runif(n = 1, min = 0, max = 1)
u

y <- ifelse(u < p, 1, 0)
y
```

::: {.callout-tip}
## Reproducibility with random numbers

Every time you run the code above, you'll get a different value for `u`
and possibly a different outcome. To make this reproducible,
we can set the random seed with e.g. `set.seed(123)`.
:::

The `runif()` function takes 3 arguments:

- `n` is the number of trials, i.e. number of coin flips
- `min` and `max` define the range, here we used the unit range $0-1$

Here is the histogram of a $Uniform(0, 1)$ random variable:

```{r}
u <- runif(n = 100, min = 0, max = 1)
summary(u)
hist(u)
```

We use the random numbers to repeat the coin flip experiment 100 times
and tabulate the results:

```{r}
ifelse(u < p, 1, 0) |>
    table(y = _) |>
    as.data.frame() |>
    mutate(Prop = Freq / sum(Freq))
```

We will see uses of the Uniform distribution later.

Another way to run the coin flip experiment in R is to use the Bernoulli
distribution.

```{r}
y <- rbinom(n = 100, size = 1, prob = p)
summary(y)
table(y = y) |>
    as.data.frame() |>
    mutate(Prop = Freq / sum(Freq))
```

The `rbinom()` function takes 3 arguments:

- `n` is the number of observations
- `size` is the number of trials
- `prob` is the probability of the outcome to be 1

The `rbinom` name refers to the Binomial distribution, which is multiple
independent Bernoulli trials. This is what the `size` argument refers to,
i.e. `size = 1` means Bernoulli.

## Simulating occupancy data

Let's see how we can use the Bernoulli distribution and the `rbinom`
function to simulate occupancy data.

Let us sample $n=50$ locations and observe the occupancy status of
Ovenbirds at each of the locations:

- The true occupancy status at the $i$th location is $W_{i}$.
- The probability of occupancy is $P(W_{i}=1)=\phi$.

The true occupancy probability $\phi` is denoted with `phi.true`:

```{r}
n <- 50
phi.true <- 0.4

W <- rbinom(n = n, size = 1, prob = phi.true)

table(W = W)
```


There are many reasons why we might not observe the true occupancy status $W$.
When the data that we collect ($Y$) is not the true occupancy status $W$, 
we say that there is detection error. It can manifest the following way:

We can capture detection error also as a probability $p$ which
is the probability of detecting the species when present ($W_i=1$).

- If the species is absent ($W_i=0$), we will always observe 0, thus $Y_i=0$: $P(Y_i=0|W_{i}=0)=1$.
- If $W_i=1$, we might observe 0 (missed all the birds) or 1:
  - species detected when present, $P(Y_i=1|W_{i}=1)=p$
  - species not detected when present, $P(Y_i=0|W_{i}=1)=1-p$

Let's simulate what we observe, the `Y` vector.
We use the `rbinom()` function with `n = n` as before,
but the size argument is not a fixed 1, but the true status `W`.
When `W` is 0, the function will always return 0,
when `W` is 1, the function will reyrun 1 or 0 according the the
detection probability $p$, denoted as `p.true`:

```{r}
p.true <- 0.6
Y <- rbinom(n = n, size = W, prob = p.true)

table(W = W, Y = Y)
```

The cross-tabulation show how many times we observed 1's vs 0's when the
true status was 1.

Using the observations as if those would represent the true status
is called the _naive_ approach. The naive approach assume that there is
no observation error:

```{r}
mean(Y)
```

But we see that the mean of `Y` is quite far from `phi.true`.

Fitting a logistic regression (aka Binomial GLM) to the data yields the
same result:

```{r}
m1 <- glm(Y ~ 1, family = binomial)
coef(m1)
plogis(coef(m1))
```

The coefficient for the intercept is the maximum likelihood estimate on the
logit scale. To transform that to the $0-1$ probability scale, we can use
the `plogis()` function that implements the inverse logistic transformation.

The unobserved `W` variable is often called a _latent_ variable.
It is latent in the sense that it cannot be directly observed.
We need to find ways to:

- use survey design to minimize the detection error, or
- use statistical models to correct for detection error, or
- the combination of the 2.

# Simulating multiple visits

A feature of multiple-visit methods is that we visit the same site $T$ times.
Each visit ($t=1,\ldots,T$) gives us a different observation, $Y_{it}$.

Key assumptions are the following:

- the visits are independent of each other
- the true status $W$ stays the same over the visits ($W_i=W_{it}$)

We can generalize the simulation code to any number of `T` visits as:

```{r}
T <- 2
Y <- matrix(NA, n, T)
for (t in 1:T) {
    Y[, t] <- rbinom(n = n, size = W, prob = p.true)
}
```

A more concise way of writing the above code without a loop is:

```{r}
#| eval: false
Y <- replicate(T, rbinom(n = n, size = W, prob = p.true))
```

If we inspect any row (site) from the `Y` matrix, we see a series of 1's and 0's
for sites where the species is present. This is called the _detection history_.

```{r}
data.frame(
    W = head(W[W > 0]),
    Visit = head(Y[W > 0, ])
)
```

The detection history is all 0's for sites where the species is absent:

```{r}
data.frame(
    W = head(W[W == 0]),
    Visit = head(Y[W == 0, ])
)
```

We can compare the maximum of the observed values over the visits at each site:

```{r}
Y_max <- apply(Y, 1, max)
table(W = W, Y_max = Y_max)
```

::: {.callout-tip}
## What happens when we increase the number of visits?

Change the value of `T` and compare the `Y_max` to `W`. 
What happens to our _naive_ estimator of using `Y_max`?
:::

```{r}
data.frame(
    T = 1:10,
    Y_max = sapply(1:10, \(T) {
        mean(apply(
            replicate(T, rbinom(n = n, size = W, prob = p.true)),
            1, max
        ))
    })
) |> ggplot(aes(x = T, y = Y_max)) +
    geom_hline(yintercept = phi.true, lty = 2, col = 2) +
    geom_line() +
    ylim(0, 1) +
    scale_x_continuous(breaks = 1:10) +
    theme_light()
```

Fit the logistic regression model to `Y_max`:

```{r}
m2 <- glm(Y_max ~ 1, family = binomial)
coef(m2)
plogis(coef(m2))
```

## Fitting occupancy models

The unmarked package implements the multiple-visit occupancy model.

::: {.callout-caution collapse="true"}
## References

Mac Kenzie et al. 2002.
Estimating site occupancy rates when detection\nprobabilities are less than one.
Ecology, 83:2248--2255.
[Fulltext](https://www.sfu.ca/~lmgonigl/materials-qm/papers/mackenzie-2002-2248.pdf)
:::

- Organize the data and an unmarked occupancy data frame
- Fit the model with the `occu`
- the `~1 ~1` formula says that we do not have covariates, just the intercepts

```{r}
library(unmarked)

umf <- unmarked::unmarkedFrameOccu(y = Y)
summary(umf)

m3 <- unmarked::occu(~1 ~ 1, umf)
m3
```

We use the `plogis()` function again to transform the estimates to the
probability scale and compare with our `phi.true` and `p.true` values:

```{r}
plogis(coef(m3, type = "det"))
plogis(coef(m3, type = "state"))
```

## Maximum likelihood estimator

The `glm()` and `occu()` functions use maximum likelihood to find the
parameter estimates for a given data set.
In other words, we coefficients (maximum likelihood estimate, or MLE)
maximize the likelihood function for the data set in question.
The likelihood function can be relatively simple and easy to calculate,
or it can be computationally challenging to compute (e.g. for hierarchical
or mixed models).

The likelihood function $L$ for the simple occupancy model with 
parameters $p$ and $\phi$ for multiple visits data can be written as:

$$L(p, \phi; y_{1,1}, \ldots, y_{n,T})  = \prod_{i=1}^{n} \left[ \phi \left( \binom{T}{y_{i \cdot}} p^{y_{i \cdot}} (1 - p)^{T - y_{i \cdot}} \right) + (1 - \phi) I(y_{i \cdot} = 0)\right]$$

where $y_{i \cdot} = \sum^{t=1}_{T} y_{i,t}$ and 
$I( y_{i \cdot} = 0 )$ is an indicator function that is equal to 1 if $y_{i \cdot} = 0$.

Here is the R code to calculate the log likelihood for the occupancy model:

```{r}
L_fun_occu <- function(Y, p, phi) {
    ydot <- rowSums(Y)
    T <- ncol(Y)
    L <- prod(
        phi *
            (choose(T, ydot) * p^ydot * (1 - p)^(T - ydot)) +
            (1 - phi) * (ydot == 0)
    )
    L
}
```

Next, we evaluate the likelihood function at different values of `p` and `phi`
while keeping the data `Y` constant. We set up the grid for this using
`expand.grid()`:

```{r}
grid <- expand.grid(
    p = seq(0, 1, by = 0.01),
    phi = seq(0, 1, by = 0.01),
    L = NA
)

for (i in 1:nrow(grid)) {
    grid$L[i] <- L_fun_occu(
        Y = Y,
        p = grid$p[i],
        phi = grid$phi[i]
    )
}
```

When we plot the likelihood surface, we see the maximum, the lines indicate
the true probability values:

```{r}
image(
    matrix(grid$L, sqrt(nrow(grid))),
    xlab = "p",
    ylab = expression(varphi)
)
abline(h = phi.true, v = p.true, col = 1, lwd = 1)

grid |> ggplot(aes(x = p, y = phi, z = L)) +
    geom_contour_filled(show.legend = FALSE) +
    geom_hline(yintercept = phi.true) +
    geom_vline(xintercept = p.true) +
    xlab("p") +
    ylab(expression(varphi)) +
    theme_light()
```

If the rgl package is installed, we can view the surface in 3D:

```{r}
if (interactive()) {
    library(rgl)
    L_mat <- matrix(grid$L, sqrt(nrow(grid)))
    open3d()
    bg3d("white")
    material3d(col = "black")
    dcpal_grbu <- colorRampPalette(c("#18bc9c", "#3498db"))
    Col <- rev(dcpal_grbu(12))[cut(L_mat, breaks = 12)]
    persp3d(L_mat / max(L_mat),
        col = Col,
        theta = 50, phi = 25, expand = 0.75, ticktype = "detailed",
        ylab = "p", xlab = "phi", zlab = "L"
    )
}
```

::: {.callout-note}
We will circle back to these plots later.
:::

# Simulating count data

Simulate N

## Observation error for counts

Crosstab of N and Y

## Count data with multiple visits

Show the max, naive etc

## Fitting N-mixture models

xxx

## N-mixture likelihood

- Multiple-visit abundance modeling
- Show the likelihood

Use qmd <https://quarto-dev.github.io/quarto-gallery/page-layout/tufte.html>
<https://raw.githubusercontent.com/quarto-dev/quarto-gallery/main/page-layout/tufte.qmd>

Hello.


<https://github.com/datacloning/workshop-2023-edmonton/tree/main/02-mixed-models>



Occupancy likelihood: eq 1, <https://www.sfu.ca/~lmgonigl/materials-qm/papers/mackenzie-2002-2248.pdf>

We use the following notation throughout this article:
ci, probability that a species is present at site i; p it,
probability that a species will be detected at site i at
time t, given presence; N, total number of surveyed

sites; T, number of distinct sampling occasions; n t num-
ber of sites where the species was detected at time t;
n., total number of sites at which the species was de-
tected at least once.



- decrease/increase n, what do you see?
- decrease/increase T, what do you see?

N-mixture:

::: {.callout-caution collapse="true"}
## References

Royle

:::

https://datacloning.org/courses/2016/madison/abundance.html
https://pmc.ncbi.nlm.nih.gov/articles/PMC4406156/

```{r}
set.seed(1234)
n <- 200
T <- 4
p.true <- 0.6
lambda.true <- 4.2
N <- rpois(n = n, lambda = lambda.true)
Y <- matrix(NA, n, T)
for (t in 1:T) {
    Y[, t] <- rbinom(n = n, size = N, prob = p.true)
}
table(N = N, Y = apply(Y, 1, max))

library(unmarked)

umf <- unmarked::unmarkedFramePCount(y = Y)
summary(umf)

fm1 <- unmarked::pcount(~1 ~ 1, umf, K = 50)
fm1

plogis(coef(fm1, type = "det"))
exp(coef(fm1, type = "state"))


L_fun <- function(Y, p, lambda, K = 50) {
    n <- nrow(Y)
    T <- ncol(Y)
    L <- rep(NA, n)
    ymax <- apply(Y, 1, max)
    for (i in 1:n) {
        S <- 0
        for (Nit in ymax[i]:K) {
            v <- 0
            for (j in 1:T) {
                v <- v + dbinom(Y[i, j], Nit, p, log = TRUE) +
                    dpois(Nit, lambda, log = TRUE)
            }
            S <- S + exp(v)
        }
        L[i] <- S
    }
    sum(log(L))
}
L_fun(Y = Y, p = p.true, lambda = lambda.true)

grid <- expand.grid(
    p = seq(0, 1, by = 0.1),
    lambda = seq(0, lambda.true * 2, by = lambda.true / 10),
    L = NA
)
for (i in 1:nrow(grid)) {
    grid$L[i] <- L_fun(Y = Y, p = grid$p[i], lambda = grid$lambda[i])
}

grid[is.finite(grid$L), ] |> ggplot(aes(x = p, y = lambda, z = L)) +
    geom_contour_filled() +
    geom_hline(yintercept = lambda.true) +
    geom_vline(xintercept = p.true) +
    xlab("p") +
    ylab(expression(lambda)) +
    theme_light()

```

We'll revisit this 
